"""
frontend.py â€” Interfaz Streamlit para el Helpdesk interno con RAG.

Ejecutar con:
    streamlit run frontend.py
"""

import streamlit as st
from backend import KnowledgeBase, rag_pipeline, EXTRACTORS

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ConfiguraciÃ³n de pÃ¡gina
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(
    page_title="ğŸ› ï¸ Helpdesk Interno",
    page_icon="ğŸ› ï¸",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Estado de sesiÃ³n
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if "kb" not in st.session_state:
    st.session_state.kb = KnowledgeBase()

if "history" not in st.session_state:
    st.session_state.history = []  # lista de {"query", "response"}

kb: KnowledgeBase = st.session_state.kb

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Sidebar â€” ConfiguraciÃ³n y carga de documentos
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with st.sidebar:
    # â”€â”€ Bloque 1: API Key Embeddings â”€â”€
    st.header("ğŸ”— Embeddings (xAI)")
    api_key_embeddings = st.text_input(
        "ğŸ”‘ API Key para Embeddings",
        type="password",
        help="API key de xAI usada para generar embeddings de documentos y consultas.",
    )

    st.divider()

    # â”€â”€ Bloque 2: API Key LLM â”€â”€
    st.header("ğŸ¤– LLM â€” Grok (xAI)")
    api_key_llm = st.text_input(
        "ğŸ”‘ API Key para Grok (LLM)",
        type="password",
        help="API key de xAI usada para generar respuestas con Grok.",
    )

    # Modelo
    model = st.selectbox(
        "Modelo Grok",
        options=["grok-3-mini", "grok-3"],
        index=0,
        help="Selecciona el modelo de Grok a utilizar.",
    )

    # Top-K
    top_k = st.slider(
        "ğŸ” Fragmentos a recuperar (top-k)",
        min_value=1,
        max_value=15,
        value=5,
        help="NÃºmero de fragmentos del contexto a enviar al LLM.",
    )

    st.divider()

    # â”€â”€ Carga de documentos â”€â”€
    st.header("ğŸ“ Base de conocimiento")

    uploaded_files = st.file_uploader(
        "Sube documentos de soporte",
        type=["pdf", "docx", "txt", "md", "log"],
        accept_multiple_files=True,
        help="Formatos soportados: PDF, DOCX, TXT, MD, LOG",
    )

    if uploaded_files:
        if not api_key_embeddings:
            st.warning("âš ï¸ Introduce la API Key de Embeddings antes de subir documentos.")
        else:
            for f in uploaded_files:
                with st.spinner(f"Procesando *{f.name}*â€¦"):
                    try:
                        n = kb.add_document(f.name, f.read(), api_key=api_key_embeddings)
                        if n > 0:
                            st.success(f"âœ… **{f.name}** â€” {n} fragmentos indexados")
                        else:
                            st.info(f"â„¹ï¸ **{f.name}** ya estaba indexado o vacÃ­o")
                    except ValueError as e:
                        st.error(f"âŒ {e}")
                    except Exception as e:
                        st.error(f"âŒ Error al generar embeddings: {e}")

    # EstadÃ­sticas de la KB
    if kb.total_chunks > 0:
        st.divider()
        st.metric("Documentos indexados", kb.total_documents)
        st.metric("Fragmentos totales", kb.total_chunks)
        with st.expander("ğŸ“„ Fuentes cargadas"):
            for src in kb.sources:
                st.write(f"- {src}")

        if st.button("ğŸ—‘ï¸ Resetear base de conocimiento", type="secondary"):
            kb.reset()
            st.session_state.kb = KnowledgeBase()
            st.rerun()
    else:
        st.caption("AÃºn no hay documentos cargados.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ãrea principal â€” Consulta de incidencias
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.title("ğŸ› ï¸ Helpdesk Interno â€” Asistente RAG")
st.caption("Describe tu incidencia tÃ©cnica y recibe una respuesta estructurada basada en la documentaciÃ³n interna.")

# Formulario de consulta
with st.form("incident_form", clear_on_submit=True):
    query = st.text_area(
        "ğŸ“ Describe tu incidencia",
        height=120,
        placeholder=(
            "Ejemplo: El usuario no puede conectarse a la VPN corporativa "
            "desde su portÃ¡til con Windows 11. Aparece el error Â«TLS handshake failedÂ»."
        ),
    )
    submitted = st.form_submit_button(
        "ğŸš€ Buscar soluciÃ³n",
        type="primary",
        use_container_width=True,
    )

# Procesar consulta
if submitted:
    # Validaciones
    if not api_key_embeddings or not api_key_llm:
        st.error("âš ï¸ Introduce ambas API Keys (Embeddings y LLM) en la barra lateral.")
    elif not query.strip():
        st.warning("âš ï¸ Escribe una descripciÃ³n de la incidencia.")
    elif kb.total_chunks == 0:
        st.warning("âš ï¸ Sube al menos un documento a la base de conocimiento antes de consultar.")
    else:
        with st.spinner("ğŸ” Buscando en la base de conocimiento y generando respuestaâ€¦"):
            try:
                result = rag_pipeline(
                    query=query.strip(),
                    kb=kb,
                    api_key_embeddings=api_key_embeddings,
                    api_key_llm=api_key_llm,
                    top_k=top_k,
                    model=model,
                )
                # Guardar en historial
                st.session_state.history.insert(0, {
                    "query":    query.strip(),
                    "answer":   result["answer"],
                    "sources":  result["sources"],
                    "context":  result["context"],
                })
            except Exception as e:
                st.error(f"âŒ Error al generar la respuesta: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Mostrar historial de respuestas
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

for i, entry in enumerate(st.session_state.history):
    with st.container(border=True):
        st.markdown(f"**ğŸ—¨ï¸ Incidencia:** {entry['query']}")
        st.divider()
        st.markdown(entry["answer"])

        # Fragmentos de contexto recuperados (colapsable)
        with st.expander("ğŸ§© Fragmentos de contexto recuperados"):
            if entry["context"]:
                for j, ctx in enumerate(entry["context"], 1):
                    st.markdown(
                        f"**Fragmento {j}** â€” `{ctx['source']}` "
                        f"(similitud: {ctx['score']:.2f})"
                    )
                    st.code(ctx["chunk"], language=None)
            else:
                st.caption("No se recuperaron fragmentos.")

    if i < len(st.session_state.history) - 1:
        st.markdown("---")
